{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t-dasnirjhar/anaconda3/envs/trl/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from scripts.reward_learning import reward_learning\n",
    "from scripts.reward_evaluation import get_accuracy\n",
    "from scripts.ppo_model_training import get_dataset_iterator, prepare_model_for_PPO, prepare_reward_model_for_PPO, prepare_PPO_config, prepare_PPO_trainer, train_model\n",
    "from scripts.model_evaluation import load_true_reward_model, load_model_and_tokenizer,  generate_response_from_model, evaluate_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anthropic + Gemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anthropic + Gemma : Small Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Gemma'\n",
    "dataset_name = 'Anthropic'\n",
    "output_folder = './output'\n",
    "small = True\n",
    "info = 'le' # identifier for large epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_epoch = 100\n",
    "epochs = 20\n",
    "id_apo = 'apo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  7.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "apo accuracy: 94.55%\n"
     ]
    }
   ],
   "source": [
    "reward_learning(dataset_name, model_name, samples_per_epoch, epochs, output_folder, id_apo, small, info)\n",
    "\n",
    "print('-'*50)\n",
    "get_accuracy(model_name, dataset_name, output_folder, small, id_apo, info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_epoch = 100\n",
    "epochs = 20\n",
    "id_rand = 'random'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 19.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "random accuracy: 83.0%\n"
     ]
    }
   ],
   "source": [
    "reward_learning(dataset_name, model_name, samples_per_epoch, epochs, output_folder, id_rand, small, info)\n",
    "\n",
    "print('-'*50)\n",
    "get_accuracy(model_name, dataset_name, output_folder, small, id_rand, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = get_dataset_iterator(dataset_name, model_name)\n",
    "model_apo, tokenizer = prepare_model_for_PPO(model_name)\n",
    "reward_model_apo, rm_tokenizer_apo = prepare_reward_model_for_PPO(model_name, dataset_name, output_folder, id_apo, info)\n",
    "ppo_config = prepare_PPO_config(model_name)\n",
    "ppo_trainer_apo, gen_kwargs = prepare_PPO_trainer(ppo_config, model_apo, tokenizer, ds)\n",
    "train_model(ppo_trainer_apo, gen_kwargs, tokenizer, reward_model_apo, rm_tokenizer_apo, output_folder, model_name, dataset_name, id_apo, max_iter=32, small=small, info=info)\n",
    "\n",
    "# Free up GPU\n",
    "del model_apo\n",
    "del tokenizer\n",
    "del reward_model_apo\n",
    "del rm_tokenizer_apo\n",
    "del ppo_config\n",
    "del ppo_trainer_apo\n",
    "del gen_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = get_dataset_iterator(dataset_name, model_name)\n",
    "model_rand, tokenizer = prepare_model_for_PPO(model_name)\n",
    "reward_model_rand, rm_tokenizer_rand = prepare_reward_model_for_PPO(model_name, dataset_name, output_folder, id_rand, info)\n",
    "ppo_config = prepare_PPO_config(model_name)\n",
    "ppo_trainer_rand, gen_kwargs = prepare_PPO_trainer(ppo_config, model_rand, tokenizer, ds)\n",
    "train_model(ppo_trainer_rand, gen_kwargs, tokenizer, reward_model_rand, rm_tokenizer_rand, output_folder, model_name, dataset_name, id_rand, max_iter=32, small=small, info=info)\n",
    "\n",
    "# Free up GPU\n",
    "del model_rand\n",
    "del tokenizer\n",
    "del reward_model_rand\n",
    "del rm_tokenizer_rand\n",
    "del ppo_config\n",
    "del ppo_trainer_rand\n",
    "del gen_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model_and_tokenizer(model_name, dataset_name, output_path=output_folder, id='apo', small=small, info=info)\n",
    "\n",
    "generate_response_from_model(model, tokenizer, dataset_name, model_name, 'apo', output_folder, small, 2000, info)\n",
    "\n",
    "del model\n",
    "del tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reward_processor = load_true_reward_model(model_name, dataset_name)\n",
    "evaluate_responses(reward_processor, dataset_name, model_name, 'apo', output_folder, small, info)\n",
    "del reward_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model_and_tokenizer(model_name, dataset_name, output_path=output_folder, id='random', small=small, info=info)\n",
    "\n",
    "generate_response_from_model(model, tokenizer, dataset_name, model_name, 'random', output_folder, small, 2000, info)\n",
    "\n",
    "del model\n",
    "del tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reward_processor = load_true_reward_model(model_name, dataset_name)\n",
    "evaluate_responses(reward_processor, dataset_name, model_name, 'random', output_folder, small, info)\n",
    "del reward_processor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
